{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import azureml.core\n",
    "from azureml.core import Workspace , Dataset\n",
    "from multiprocessing import cpu_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweets Sentiment analysis\n",
    "# Loading and preprocessing data\n",
    "- apply stemmer/ lemmatizer\n",
    "- select target and text columns\n",
    "- remove URL\n",
    "- remove punctuation\n",
    "- remove rows with Nan\n",
    "- remove stopwords\n",
    "- tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      awww that a bummer you shoulda got david car...\n",
       "1    is upset that he can t updat hi facebook by te...\n",
       "2    i dive mani time for the ball manag to save  t...\n",
       "3         my whole bodi feel itchi and like it on fire\n",
       "4    no it not behav at all i m mad whi am i here b...\n",
       "Name: Col6, dtype: object"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset= pd.read_csv(\"tw.csv\", names=[\"Col1\",\"Col2\",\"Col3\",\"Col4\",\"Col5\",\"Col6\"], encoding='ISO-8859-1') \n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem import PorterStemmer \n",
    "import string\n",
    "import re\n",
    "\n",
    "##############################################################\n",
    "##STEMMER##\n",
    "ps = PorterStemmer()\n",
    "dataset['Col6']=dataset['Col6'].apply(lambda x: \" \".join(ps.stem(i) for i in tknzr.tokenize(x) ))\n",
    "\n",
    "###############################################################\n",
    "\n",
    "################################################################\n",
    "#LEMMATIZER##\n",
    "#lemmatizer = WordNetLemmatizer()\n",
    "#dataset['Col6']=dataset['Col6'].apply(lambda x: \" \".join(lemmatizer.lemmatize(i) for i in tknzr.tokenize(x) ))\n",
    "#################################################################\n",
    "dataset['Col6']=dataset['Col6'].apply(lambda x: re.sub(\"@S+|www?:\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\",' ',x))\n",
    "dataset= dataset.dropna(axis=0, how=\"any\")\n",
    "dataset['Col6']=dataset['Col6'].str.replace('\\d+', '').apply(lambda x:str(x).translate(str.maketrans('', '', string.punctuation)).lower().rstrip())\n",
    "dataset['Col6'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Col1</th>\n",
       "      <th>Col6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>awww that a bummer you shoulda got david car...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can t updat hi facebook by te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>i dive mani time for the ball manag to save  t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole bodi feel itchi and like it on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>no it not behav at all i m mad whi am i here b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Col1                                               Col6\n",
       "0     0    awww that a bummer you shoulda got david car...\n",
       "1     0  is upset that he can t updat hi facebook by te...\n",
       "2     0  i dive mani time for the ball manag to save  t...\n",
       "3     0       my whole bodi feel itchi and like it on fire\n",
       "4     0  no it not behav at all i m mad whi am i here b..."
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset= dataset[[\"Col1\",\"Col6\"]]\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "stop_words = set(stopwords.words('english')) \n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1d9de94b6c8>"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset[\"Col1\"].plot(kind=\"hist\", title=\"Target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [awww, bummer, shoulda, got, david, carr, thir...\n",
       "1    [upset, updat, hi, facebook, text, might, cri,...\n",
       "2    [dive, mani, time, ball, manag, save, rest, go...\n",
       "3               [whole, bodi, feel, itchi, like, fire]\n",
       "4                       [behav, mad, whi, becaus, see]\n",
       "Name: Col6, dtype: object"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['Col6'] = dataset['Col6'].apply(lambda x: [w for w in word_tokenize(x) if not w in stop_words])\n",
    "dataset['Col6'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple model\n",
    "\n",
    "Word vectors has been averaged into 1D corpus representation\n",
    "- apply fasttext and word2vec algorithms\n",
    "- build vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds=dataset\n",
    "from gensim.models import Word2Vec, FastText\n",
    "############FASTTEXT#############\n",
    "model = FastText(word_tokens, min_count=1, size=8,workers=cpu_count())\n",
    "#################################\n",
    "\n",
    "##############Word2Vec###########\n",
    "#model = Word2Vec(word_tokens, min_count=1, size=8,workers=cpu_count())\n",
    "################################\n",
    "word_tokens=ds['Col6']\n",
    "words = list(model.wv.vocab)\n",
    "print(words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- average vectors\n",
    "- replace words with average of vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "214918"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic={}\n",
    "ww=list(model.wv.vocab)\n",
    "for i in ww:\n",
    "    dic.update({i:model.wv[i].mean()})\n",
    "len(dic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "final=[]\n",
    "for i in ds[\"Col6\"]:\n",
    "    arr=[]\n",
    "    for u in i:\n",
    "        if u in dic.keys():\n",
    "            arr.append(dic[u])\n",
    "        else:\n",
    "            print(u)\n",
    "    final.append(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- keep 20 words per tweet, to have equal matrix of documents\n",
    "- check if all the documents have the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keeping 20 words per tweet\n",
    "\n",
    "for i in final:\n",
    "    if len(i)<21:\n",
    "        for b in range(len(i),20):\n",
    "            i.append(0)\n",
    "    elif len(i)>20:\n",
    "        b=len(i)\n",
    "        while b>20:\n",
    "            i.pop(b-1)\n",
    "            b=b-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "#check nr tweets\n",
    "max=0\n",
    "min=90\n",
    "for i in final:\n",
    "    if len(i)>max:\n",
    "        max=len(i)\n",
    "    if len(i)<min:\n",
    "        min=len(i)\n",
    "print(min)\n",
    "print(max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600000, 20)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X= pd.DataFrame(final)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600000,)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['Col1']= ds['Col1'].replace(4,1)\n",
    "Y=ds['Col1']\n",
    "Y.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- define DNN model \n",
    "- split dataset into test and train sets\n",
    "- fit and run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n",
      "Train on 1024000 samples, validate on 256000 samples\n",
      "Epoch 1/10\n",
      "1024000/1024000 [==============================] - 241s 236us/step - loss: 0.6742 - accuracy: 0.5680 - val_loss: 0.6707 - val_accuracy: 0.5746\n",
      "Epoch 2/10\n",
      "1024000/1024000 [==============================] - 248s 242us/step - loss: 0.6707 - accuracy: 0.5743 - val_loss: 0.6698 - val_accuracy: 0.5749\n",
      "Epoch 3/10\n",
      "1024000/1024000 [==============================] - 249s 243us/step - loss: 0.6695 - accuracy: 0.5768 - val_loss: 0.6687 - val_accuracy: 0.5758\n",
      "Epoch 4/10\n",
      "1024000/1024000 [==============================] - 244s 238us/step - loss: 0.6690 - accuracy: 0.5766 - val_loss: 0.6692 - val_accuracy: 0.5750\n",
      "Epoch 5/10\n",
      "1024000/1024000 [==============================] - 236s 231us/step - loss: 0.6685 - accuracy: 0.5781 - val_loss: 0.6679 - val_accuracy: 0.5789\n",
      "Epoch 6/10\n",
      "1024000/1024000 [==============================] - 240s 235us/step - loss: 0.6686 - accuracy: 0.5790 - val_loss: 0.6675 - val_accuracy: 0.5807\n",
      "Epoch 7/10\n",
      "1024000/1024000 [==============================] - 240s 235us/step - loss: 0.6682 - accuracy: 0.5794 - val_loss: 0.6680 - val_accuracy: 0.5788\n",
      "Epoch 8/10\n",
      "1024000/1024000 [==============================] - 248s 243us/step - loss: 0.6680 - accuracy: 0.5799 - val_loss: 0.6679 - val_accuracy: 0.5805\n",
      "Epoch 9/10\n",
      "1024000/1024000 [==============================] - 228s 223us/step - loss: 0.6678 - accuracy: 0.5806 - val_loss: 0.6679 - val_accuracy: 0.5812\n",
      "Epoch 10/10\n",
      "1024000/1024000 [==============================] - 239s 234us/step - loss: 0.6678 - accuracy: 0.5806 - val_loss: 0.6676 - val_accuracy: 0.5819\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1daab5d6348>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "# Import `Sequential` from `keras.models`\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Import `Dense` from `keras.layers`\n",
    "from keras.layers import Dense\n",
    "from keras.layers import SimpleRNN\n",
    "\n",
    "# Initialize the constructor\n",
    "model = Sequential()\n",
    "# Add an input layer \n",
    "model.add(Dense(20, activation='relu', input_shape=(20,)))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "\n",
    "model.add(Dense(12, activation='relu'))\n",
    "# Add one hidden layer \n",
    "model.add(Dense(8, activation='relu'))\n",
    "# Add an output layer \n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "model.fit(X_train, y_train,epochs= 10, batch_size=5, verbose=1, validation_split=0.2 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320000/320000 [==============================] - 5s 15us/step\n",
      "[0.6679766739666462, 0.5808968544006348]\n"
     ]
    }
   ],
   "source": [
    "acc= model.evaluate(X_test,y_test)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5813331548131104"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_scores = model.predict_classes(X_test)\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_test, y_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Basic Model\n",
    "\n",
    "<table><th>Model</th><th>Word ending</th><th>Vectorization</th><th>Accuracy</th><th>Loss</th><th>AUC</th><th>Max Time</th>\n",
    "    <tr>\n",
    "        <td>Input20/Dense16/Dense12/Dense8/Output1</td><td>Lemmatization</td><td>Word2vec<td>0.58</td><td>0.67</td><td>0.58</td><td>193</td>\n",
    "        </tr><tr>\n",
    "    <td>Input20/Dense16/Dense12/Dense8/Output1</td><td>Lemmatization</td><td>FastText<td>0.55</td><td>0.68</td><td>0.55</td><td>244</td></tr><tr>\n",
    "    <td>Input20/Dense16/Dense12/Dense8/Output1</td><td>Stemming</td><td>FastText<td>0.58</td><td>0.67</td><td>0.58</td><td>249</td>\n",
    "     </tr><tr>\n",
    "    <td>Input20/Dense16/Dense12/Dense8/Output1</td><td>Stemming</td><td>Word2vec<td>0.58</td><td>0.67</td><td>0.58</td><td>190</td>\n",
    "     </tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple dimentions model\n",
    "Keeping 100 word vectors and loading them into Embeding layer of keras model\n",
    "- preprocessing documents\n",
    "- keep target and text column\n",
    "- apply lemmitizer or stemmer\n",
    "- remove stopwords\n",
    "- remove URI\n",
    "- remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset= pd.read_csv(\"tw.csv\", names=[\"Col1\",\"Col2\",\"Col3\",\"Col4\",\"Col5\",\"Col6\"], encoding='ISO-8859-1') \n",
    "#data preprocessing- done\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem import PorterStemmer \n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from multiprocessing import cpu_count\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "############ LEMMITIZER ###############\n",
    "#lemmatizer = WordNetLemmatizer()\n",
    "#dataset['Col6']=dataset['Col6'].apply(lambda x: \" \".join(lemmatizer.lemmatize(i) for i in tknzr.tokenize(x) if not i in stop_words ))\n",
    "#######################################\n",
    "\n",
    "########### STEMMER ##################\n",
    "ps = PorterStemmer()\n",
    "dataset['Col6']=dataset['Col6'].apply(lambda x: \" \".join(ps.stem(i) for i in tknzr.tokenize(x) if not i in stop_words ))\n",
    "######################################\n",
    "\n",
    "dataset['Col6']=dataset['Col6'].apply(lambda x: re.sub(\"@S+|www?:\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\",' ',x))\n",
    "dataset['Col1']= dataset['Col1'].replace(4,1)\n",
    "dataset['Col6']=dataset['Col6'].str.replace('\\d+', '').apply(lambda x:str(x).translate(str.maketrans('', '', string.punctuation)).lower().rstrip())\n",
    "dataset= dataset[[\"Col1\",\"Col6\"]]\n",
    "dataset= dataset.dropna(axis=0,how=\"any\")\n",
    "ds= dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tokenize documents\n",
    "- apply end of word processing algorithm word2vec /fasttex\n",
    "- create vocabulary\n",
    "- create 100 dimentional matrix that represents the distionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import cpu_count\n",
    "from gensim.models import FastText, Word2Vec\n",
    "\n",
    "tokenized= ds\n",
    "word_tokens= tokenized['Col6'].apply(lambda x: [w for w in word_tokenize(x)])\n",
    "nrDimentsions=100\n",
    "\n",
    "######## Word2Text ####################\n",
    "#model = Word2Vec(word_tokens, min_count=1, size=nrDimentsions,workers=cpu_count())\n",
    "#######################################\n",
    "\n",
    "######## FastText ######################\n",
    "model = FastText(word_tokens, min_count=1, size=nrDimentsions,workers=cpu_count())\n",
    "########################################\n",
    "\n",
    "words = list(model.wv.vocab)\n",
    "print(words)\n",
    "\n",
    "# save the vectors in a new matrix\n",
    "embedding_matrix = np.zeros((len(model.wv.vocab) + 1, nrDimentsions))\n",
    "for i, vec in enumerate(model.wv.vectors):\n",
    "    embedding_matrix[i] = vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - Effective 'alpha' higher than previous training cycles\n"
     ]
    }
   ],
   "source": [
    "model.train(word_tokens,total_examples=len(word_tokens),epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- define DNN model with multidimentional input\n",
    "- prepare tokens for model loading\n",
    "- split data into train and test\n",
    "- save tokenizer to file\n",
    "- fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# how many features should the tokenizer extract\n",
    "features = embedding_matrix.shape[0]\n",
    "tokenizer = Tokenizer(num_words = features)\n",
    "# fit the tokenizer on our text\n",
    "tokenizer.fit_on_texts(ds['Col6'])\n",
    "\n",
    "# get all words that the tokenizer knows\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# put the tokens in a matrix\n",
    "X = tokenizer.texts_to_sequences(ds['Col6'])\n",
    "X = pad_sequences(X)\n",
    "\n",
    "# prepare the labels\n",
    "y = pd.get_dummies(ds['Col1'])\n",
    "\n",
    "# split in train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, ds['Col1'], test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- save tokenizer\n",
    "- fit model\n",
    "- calculate mertics\n",
    "- save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# saving tokenizer to file\n",
    "with open('tokenizer.pkl', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600000, 42)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Embedding, Dropout, Activation,Conv1D,MaxPooling1D\n",
    "\n",
    "# init model\n",
    "mod = Sequential()\n",
    "# emmbed word vectors\n",
    "mod.add(Embedding(len(model.wv.vocab)+1,nrDimentsions,input_length=X.shape[1],weights=[embedding_matrix],trainable=False))\n",
    "mod.add(Dropout(0.25))\n",
    "\n",
    "#mod.add(MaxPooling1D(pool_size=4))\n",
    "mod.add(LSTM(200))\n",
    "\n",
    "mod.add(Dense(1))\n",
    "mod.add(Activation('sigmoid'))\n",
    "\n",
    "\n",
    "mod.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.18 ms\n",
      "Train on 1152000 samples, validate on 288000 samples\n",
      "Epoch 1/2\n",
      "1152000/1152000 [==============================] - 5756s 5ms/step - loss: 0.5379 - accuracy: 0.7239 - val_loss: 0.5115 - val_accuracy: 0.7447\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lesh3\\Anaconda3\\lib\\site-packages\\keras\\callbacks\\callbacks.py:846: RuntimeWarning: Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: val_loss,val_accuracy,loss,accuracy,lr\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1152000/1152000 [==============================] - 6530s 6ms/step - loss: 0.5260 - accuracy: 0.7334 - val_loss: 0.5062 - val_accuracy: 0.7489\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1dc5b4b92c8>"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "mod.fit(X_train,y_train,epochs= 2, batch_size=5, verbose=1, validation_split=0.2, callbacks= [ ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0),\n",
    "              EarlyStopping(monitor='val_acc', min_delta=1e-4, patience=5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160000/160000 [==============================] - 101s 633us/step\n",
      "[0.5059733984947204, 0.7489562630653381]\n"
     ]
    }
   ],
   "source": [
    "acc= mod.evaluate(X_test,y_test)\n",
    "print(acc)\n",
    "\n",
    "y_scores = mod.predict_classes(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = mod.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "mod.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7488592621377752"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Multiple Dimentions Input Model\n",
    "\n",
    "<table><th>Model</th><th>Word ending</th><th>Vectorization</th><th>Accuracy</th><th>Loss</th><th>AUC</th><th>Max Time</th>\n",
    "    <tr>\n",
    "        <td>Embedding/Dropout0.25/LSMT200/Output1</td><td>Lemmatization</td><td>Word2vec<td>0.7594</td><td>0.49</td><td>0.7595</td><td>5203</td>\n",
    "        </tr><tr>\n",
    "    <td>Embedding/Dropout0.25/LSMT200/Output1</td><td>Lemmatization</td><td>FastText<td>0.749</td><td>0.506</td><td>0.7489</td><td>6530</td></tr><tr>\n",
    "    <td>Embedding/Dropout0.25/LSMT200/Output1</td><td>Stemming</td><td>FastText<td>0.748</td><td>0.508</td><td>0.748</td><td>5629</td>\n",
    "     </tr><tr>\n",
    "    <td>Embedding/Dropout0.25/LSMT200/Output1</td><td>Stemming</td><td>Word2vec<td>0.762</td><td>0.48</td><td>0.763</td><td>5408</td>\n",
    "     </tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commercial API Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, \"@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\")\n",
      "b'{\"documents\":[{\"id\":\"1\",\"score\":0.87795019149780273}],\"errors\":[]}'\n",
      "(1, \"is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\")\n",
      "b'{\"documents\":[{\"id\":\"1\",\"score\":0.11837613582611084}],\"errors\":[]}'\n",
      "(2, '@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds')\n",
      "b'{\"documents\":[{\"id\":\"1\",\"score\":0.8160826563835144}],\"errors\":[]}'\n",
      "(3, 'my whole body feels itchy and like its on fire ')\n",
      "b'{\"documents\":[{\"id\":\"1\",\"score\":0.056455731391906738}],\"errors\":[]}'\n",
      "(4, \"@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there. \")\n",
      "b'{\"documents\":[{\"id\":\"1\",\"score\":0.026735126972198486}],\"errors\":[]}'\n",
      "(5, '@Kwesidei not the whole crew ')\n",
      "b'{\"documents\":[{\"id\":\"1\",\"score\":0.720441460609436}],\"errors\":[]}'\n",
      "(6, 'Need a hug ')\n",
      "b'{\"documents\":[{\"id\":\"1\",\"score\":0.15054789185523987}],\"errors\":[]}'\n",
      "(7, \"@LOLTrish hey  long time no see! Yes.. Rains a bit ,only a bit  LOL , I'm fine thanks , how's you ?\")\n",
      "b'{\"documents\":[{\"id\":\"1\",\"score\":0.9729386568069458}],\"errors\":[]}'\n",
      "(8, \"@Tatiana_K nope they didn't have it \")\n",
      "b'{\"documents\":[{\"id\":\"1\",\"score\":0.030338048934936523}],\"errors\":[]}'\n"
     ]
    }
   ],
   "source": [
    "import http.client, urllib.request, urllib.parse, urllib.error, base64, json\n",
    "datas= pd.read_csv(\"tw.csv\", names=[\"Col1\",\"Col2\",\"Col3\",\"Col4\",\"Col5\",\"Col6\"], encoding='ISO-8859-1')\n",
    "counter=0\n",
    "for i in datas['Col6'].iteritems():\n",
    "    counter=counter+1\n",
    "    if counter==10:\n",
    "        break\n",
    "    else:\n",
    "        headers = {\n",
    "    # Request headers\n",
    "    'Content-Type': 'application/json',\n",
    "    'Ocp-Apim-Subscription-Key': 'key',\n",
    "}\n",
    "\n",
    "        params = urllib.parse.urlencode({\n",
    "    # Request parameters\n",
    "    'showStats': '{true}',\n",
    "    'model-version': '{2}',\n",
    "})\n",
    "\n",
    "        try:\n",
    "            conn = http.client.HTTPSConnection('westcentralus.api.cognitive.microsoft.com')\n",
    "            conn.request(\"POST\", \"/text/analytics/v2.0/sentiment?%s\" % params, json.dumps(\n",
    "  {\"documents\": [\n",
    "    {\n",
    "      \"language\": \"en\",\n",
    "      \"id\": \"1\",\n",
    "      \"text\": str(i)\n",
    "    }]}), headers)\n",
    "            response = conn.getresponse()\n",
    "            data = response.read()\n",
    "            print(i)\n",
    "            print(data)\n",
    "            conn.close()\n",
    "        except Exception as e:\n",
    "            print(\"[Errno {0}] {1}\".format(e.errno, e.strerror))\n",
    "    \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customized Model Deployed Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\n",
      "0:00:00.656637\n",
      "Positive\n",
      "*******************************************************\n",
      "is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\n",
      "0:00:00.308153\n",
      "Negative\n",
      "*******************************************************\n",
      "@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds\n",
      "0:00:00.263334\n",
      "Negative\n",
      "*******************************************************\n",
      "my whole body feels itchy and like its on fire \n",
      "0:00:00.251848\n",
      "Negative\n",
      "*******************************************************\n",
      "@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there. \n",
      "0:00:00.287267\n",
      "Negative\n",
      "*******************************************************\n",
      "@Kwesidei not the whole crew \n",
      "0:00:00.282035\n",
      "Positive\n",
      "*******************************************************\n",
      "Need a hug \n",
      "0:00:00.257519\n",
      "Negative\n",
      "*******************************************************\n",
      "@LOLTrish hey  long time no see! Yes.. Rains a bit ,only a bit  LOL , I'm fine thanks , how's you ?\n",
      "0:00:00.259252\n",
      "Positive\n",
      "*******************************************************\n",
      "@Tatiana_K nope they didn't have it \n",
      "0:00:00.275651\n",
      "Positive\n",
      "*******************************************************\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "scoring_uri = 'http://b713b98a-5ead-4c2b-ba9a-9ff4682b1712.eastus.azurecontainer.io/score'\n",
    "headers = {'Content-Type':'application/json'}\n",
    "counter=0\n",
    "for i in datas['Col6'].iteritems():\n",
    "    counter=counter+1\n",
    "    if counter==10:\n",
    "        break\n",
    "    else:\n",
    "        print(i[1])\n",
    "        test_data = json.dumps({'text': i[1]})\n",
    "\n",
    "        response = requests.post(scoring_uri, data=test_data, headers=headers)\n",
    "       \n",
    "        print(response.elapsed)\n",
    "        print(response.json())\n",
    "        print(\"*******************************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
